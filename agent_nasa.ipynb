{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NASA Mars Missions Q&A with Azure AI Agent\n",
        "This notebook demonstrates how to:\n",
        "- Connect to Azure AI Projects using `AIProjectClient`.\n",
        "- Build an Agent that can search NASA documents about Mars missions.\n",
        "- Use Bing search as a fallback for grounding.\n",
        "- Call custom Python functions.\n",
        "- Present a user-friendly chat UI with Gradio.\n",
        "\n",
        "> **Prerequisites**:\n",
        ">\n",
        "> 1. An Azure AI Projects (preview) resource (your `PROJECT_CONNECTION_STRING`).\n",
        "> 2. A Bing search connection (your `BING_CONNECTION_NAME`).\n",
        "> 3. A local folder `nasa-data` with at least one NASA public-domain text file about Mars.\n",
        "> 4. A `.env` file (optional) containing the relevant environment variables.\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install & Import Packages\n",
        "If you haven't installed these packages, do so with:\n",
        "```bash\n",
        "pip install azure-ai-projects azure-identity gradio python-dotenv\n",
        "```\n",
        "Then, run the imports below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from datetime import datetime as pydatetime\n",
        "from typing import Any, List, Dict\n",
        "\n",
        "import gradio as gr\n",
        "from gradio import ChatMessage\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import (\n",
        "    AgentEventHandler,\n",
        "    RunStep,\n",
        "    RunStepDeltaChunk,\n",
        "    ThreadMessage,\n",
        "    ThreadRun,\n",
        "    MessageDeltaChunk,\n",
        "    BingGroundingTool,\n",
        "    FilePurpose,\n",
        "    FileSearchTool,\n",
        "    FunctionTool,\n",
        "    ToolSet\n",
        ")\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) (Optional) Define Custom Python Functions\n",
        "You might have custom logic, such as looking up upcoming rocket launches or formatting a mission summary. We'll define an example function below.\n",
        "\n",
        "In a real project, you could have a separate file `mars_functions.py` or something similar. For simplicity, we'll define one inline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_upcoming_rocket_launches():\n",
        "    \"\"\"\n",
        "    A placeholder function that returns a short message about upcoming NASA rocket launches.\n",
        "    In reality, you'd integrate an official NASA API or a known schedule.\n",
        "    \"\"\"\n",
        "    return \"The next NASA rocket launch is scheduled for March 10, 2025, from Kennedy Space Center.\"  # example data\n",
        "\n",
        "def format_mission_summary(mission_name: str, highlight: str):\n",
        "    \"\"\"\n",
        "    A placeholder function that returns a formatted string summary about a mission.\n",
        "    \"\"\"\n",
        "    return f\"Mission {mission_name}: Key highlight -> {highlight}\"  # example data\n",
        "\n",
        "# We'll combine these into a dictionary so we can wrap them in a FunctionTool.\n",
        "mars_functions = {\n",
        "    \"get_upcoming_rocket_launches\": get_upcoming_rocket_launches,\n",
        "    \"format_mission_summary\": format_mission_summary,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Create Azure AI Client\n",
        "Use `DefaultAzureCredential` (which can work with Azure CLI, VS Code, managed identity, etc.) plus your `PROJECT_CONNECTION_STRING` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient.from_connection_string(\n",
        "    credential=credential,\n",
        "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]  # e.g. 'endpoint=https://xxx;accesskey=xxxx'\n",
        ")\n",
        "project_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Set Up Tools\n",
        "We'll attempt to connect to Bing and create/reuse a vector store for our NASA docs. \n",
        "\n",
        "### 4.1) Bing Grounding Tool\n",
        "Requires an existing Bing connection name set in your environment variable `BING_CONNECTION_NAME`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "try:\n",
        "    bing_connection = project_client.connections.get(connection_name=os.environ[\"BING_CONNECTION_NAME\"])\n",
        "    bing_tool = BingGroundingTool(connection_id=bing_connection.id)\n",
        "    print(\"bing > connected successfully\")\n",
        "except Exception as e:\n",
        "    print(\"bing > not connected; check your BING_CONNECTION_NAME.\")\n",
        "    print(e)\n",
        "    bing_tool = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2) File Search Tool (Vector Store)\n",
        "We'll store NASA documents in a vector store named `mars-vector-store`. If it doesn’t exist, we’ll upload documents from `nasa-data/` and create a store."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "VECTOR_STORE_NAME = \"mars-vector-store\"\n",
        "FOLDER_NAME = \"nasa-data\"  # local folder with text or PDF docs about NASA / Mars\n",
        "\n",
        "all_stores = project_client.agents.list_vector_stores().data\n",
        "existing_store = next((s for s in all_stores if s.name == VECTOR_STORE_NAME), None)\n",
        "\n",
        "vector_store_id = None\n",
        "if existing_store:\n",
        "    vector_store_id = existing_store.id\n",
        "    print(f\"Reusing vector store: {existing_store.name} (id: {vector_store_id})\")\n",
        "else:\n",
        "    if os.path.isdir(FOLDER_NAME):\n",
        "        file_ids = []\n",
        "        for fname in os.listdir(FOLDER_NAME):\n",
        "            fpath = os.path.join(FOLDER_NAME, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                print(f\"Uploading file: {fname}\")\n",
        "                uploaded_file = project_client.agents.upload_file_and_poll(\n",
        "                    file_path=fpath,\n",
        "                    purpose=FilePurpose.AGENTS\n",
        "                )\n",
        "                file_ids.append(uploaded_file.id)\n",
        "\n",
        "        if file_ids:\n",
        "            print(f\"Creating vector store from {len(file_ids)} file(s)...\")\n",
        "            vs = project_client.agents.create_vector_store_and_poll(\n",
        "                file_ids=file_ids,\n",
        "                name=VECTOR_STORE_NAME\n",
        "            )\n",
        "            vector_store_id = vs.id\n",
        "            print(f\"Vector store created: {vs.name} (id: {vector_store_id})\")\n",
        "    else:\n",
        "        print(f\"No folder '{FOLDER_NAME}' found. Create it and add NASA docs.\")\n",
        "\n",
        "file_search_tool = None\n",
        "if vector_store_id:\n",
        "    file_search_tool = FileSearchTool(vector_store_ids=[vector_store_id])\n",
        "    print(\"file_search_tool > ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Combine Tools Into a ToolSet\n",
        "We’ll create a `ToolSet` subclass that logs each tool call, then add our tools:\n",
        "- BingGroundingTool\n",
        "- FileSearchTool (vector store)\n",
        "- FunctionTool (our custom Python functions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LoggingToolSet(ToolSet):\n",
        "    def execute_tool_calls(self, tool_calls: List[Any]) -> List[dict]:\n",
        "        # Before calling the parent logic, log the input arguments\n",
        "        for call in tool_calls:\n",
        "            if hasattr(call, 'function') and call.function:\n",
        "                fn_name = call.function.name\n",
        "                fn_args = call.function.arguments\n",
        "                print(f\"[Tool Call Start] {fn_name} with args: {fn_args}\")\n",
        "\n",
        "        results = super().execute_tool_calls(tool_calls)\n",
        "\n",
        "        # Log the output results\n",
        "        for r in results:\n",
        "            print(f\"[Tool Call Result] {r['output']}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "custom_functions_tool = FunctionTool(mars_functions)\n",
        "\n",
        "toolset = LoggingToolSet()\n",
        "if bing_tool:\n",
        "    toolset.add(bing_tool)\n",
        "if file_search_tool:\n",
        "    toolset.add(file_search_tool)\n",
        "toolset.add(custom_functions_tool)\n",
        "\n",
        "# Optional: print out tool details\n",
        "for t in toolset._tools:\n",
        "    print(f\"Tool: {t.__class__.__name__}\")\n",
        "    for d in t.definitions:\n",
        "        if hasattr(d, 'function'):\n",
        "            print(f\" - {d.function.name} => {d.function.description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Create (or Reuse) Agent\n",
        "We'll make an agent named `nasa-mars-agent` using an Azure model (like `gpt-4o`, `gpt-3.5`, etc.). \n",
        "If it already exists, we'll reuse it. Otherwise, we'll create a new one with instructions describing the role."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "AGENT_NAME = \"nasa-mars-agent\"\n",
        "all_agents = project_client.agents.list_agents().data\n",
        "found_agent = next((a for a in all_agents if a.name == AGENT_NAME), None)\n",
        "\n",
        "model_deployment = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
        "system_instructions = (\n",
        "    \"You are a helpful assistant specialized in NASA Mars missions. \"\n",
        "    f\"Today is {pydatetime.now().strftime('%Y-%m-%d')}. \"\n",
        "    \"You can reference NASA docs in file_search, use Bing for grounding, \"\n",
        "    \"and call custom functions like get_upcoming_rocket_launches. \"\n",
        "    \"Provide concise, factual, and professional responses.\"\n",
        ")\n",
        "\n",
        "if found_agent:\n",
        "    print(f\"Reusing existing agent: {found_agent.name} (id: {found_agent.id})\")\n",
        "    agent = project_client.agents.update_agent(\n",
        "        assistant_id=found_agent.id,\n",
        "        model=found_agent.model,\n",
        "        instructions=found_agent.instructions,\n",
        "        toolset=toolset,\n",
        "    )\n",
        "else:\n",
        "    agent = project_client.agents.create_agent(\n",
        "        model=model_deployment,\n",
        "        name=AGENT_NAME,\n",
        "        instructions=system_instructions,\n",
        "        toolset=toolset\n",
        "    )\n",
        "    print(f\"Created new agent: {agent.name} (id: {agent.id})\")\n",
        "\n",
        "agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Create a Conversation Thread\n",
        "A conversation *thread* is like a chat session with the agent. We can have multiple threads for different chat sessions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "thread = project_client.agents.create_thread()\n",
        "print(f\"Created thread with id: {thread.id}\")\n",
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Define a Custom Event Handler for Streaming\n",
        "We'll implement `AgentEventHandler` to see partial responses, which helps for debugging. \n",
        "For production, you might skip or simplify the console logging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MyEventHandler(AgentEventHandler):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._current_message_id = None\n",
        "        self._accumulated_text = \"\"\n",
        "\n",
        "    def on_message_delta(self, delta: MessageDeltaChunk) -> None:\n",
        "        # If a new message, we start fresh\n",
        "        if delta.id != self._current_message_id:\n",
        "            if self._current_message_id is not None:\n",
        "                print()  # new line from the previous message\n",
        "            self._current_message_id = delta.id\n",
        "            self._accumulated_text = \"\"\n",
        "            print(\"\\nAssistant says:\", end=\" \")\n",
        "\n",
        "        # Accumulate partial text\n",
        "        partial_text = \"\"\n",
        "        if delta.delta.content:\n",
        "            for chunk in delta.delta.content:\n",
        "                partial_text += chunk.text.get(\"value\", \"\")\n",
        "\n",
        "        self._accumulated_text += partial_text\n",
        "        print(partial_text, end=\"\", flush=True)\n",
        "\n",
        "    def on_thread_message(self, message: ThreadMessage) -> None:\n",
        "        if message.status == \"completed\" and message.role == \"assistant\":\n",
        "            print(\"\\n[Assistant message complete]\\n\")\n",
        "            self._current_message_id = None\n",
        "            self._accumulated_text = \"\"\n",
        "\n",
        "    def on_thread_run(self, run: ThreadRun) -> None:\n",
        "        print(f\"\\nThreadRun status: {run.status}\")\n",
        "        if run.status == \"failed\":\n",
        "            print(f\"[Error] {run.last_error}\")\n",
        "\n",
        "    def on_run_step(self, step: RunStep) -> None:\n",
        "        print(f\"[RunStep] {step.type} => {step.status}\")\n",
        "\n",
        "    def on_run_step_delta(self, delta: RunStepDeltaChunk) -> None:\n",
        "        pass  # You could handle partial tool calls here.\n",
        "\n",
        "    def on_unhandled_event(self, event_type: str, event_data):\n",
        "        pass  # for debugging any unexpected events\n",
        "\n",
        "    def on_error(self, data: str) -> None:\n",
        "        print(f\"[Stream Error] {data}\")\n",
        "\n",
        "    def on_done(self) -> None:\n",
        "        print(\"[Streaming done]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Main Chat Function for Gradio\n",
        "We'll define a function `mars_agent_chat` that:\n",
        "1. Takes user input and existing chat history (as lists of `dict` messages in Gradio format).\n",
        "2. Sends the user message to the agent.\n",
        "3. Streams partial responses back (including function/tool calls), building an updated conversation.\n",
        "\n",
        "We then yield partial updates to the Gradio UI in real-time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _convert_dict_to_chatmessage(msg: dict) -> ChatMessage:\n",
        "    return ChatMessage(role=msg[\"role\"], content=msg[\"content\"], metadata=msg.get(\"metadata\", None))\n",
        "\n",
        "def mars_agent_chat(user_message: str, history: List[dict]):\n",
        "    # Convert Gradio history to ChatMessage objects\n",
        "    conversation = [_convert_dict_to_chatmessage(m) for m in history]\n",
        "\n",
        "    # Add the new user message\n",
        "    conversation.append(ChatMessage(role=\"user\", content=user_message))\n",
        "    yield (conversation, \"\")  # Show the user message and clear the input box\n",
        "\n",
        "    # Post the user's message to the Azure AI thread\n",
        "    project_client.agents.create_message(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=user_message\n",
        "    )\n",
        "\n",
        "    # We'll track partial tool calls or partial text. This is done by streaming events below.\n",
        "\n",
        "    with project_client.agents.create_stream(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=agent.id,\n",
        "        event_handler=MyEventHandler()\n",
        "    ) as stream:\n",
        "        for item in stream:\n",
        "            event_type, event_data, *_ = item\n",
        "\n",
        "            # 1) If the agent is sending partial text, handle it\n",
        "            if event_type == \"thread.message.delta\":\n",
        "                text_delta = \"\"\n",
        "                for chunk in event_data[\"delta\"][\"content\"]:\n",
        "                    text_delta += chunk[\"text\"].get(\"value\", \"\")\n",
        "\n",
        "                # If the last message is an assistant bubble, append partial text\n",
        "                if conversation and conversation[-1].role == \"assistant\":\n",
        "                    conversation[-1].content += text_delta\n",
        "                else:\n",
        "                    # create a new assistant message bubble\n",
        "                    conversation.append(ChatMessage(role=\"assistant\", content=text_delta))\n",
        "                yield (conversation, \"\")\n",
        "\n",
        "            # 2) If entire assistant message is completed\n",
        "            elif event_type == \"thread.message\" and event_data[\"role\"] == \"assistant\" and event_data[\"status\"] == \"completed\":\n",
        "                # finalize that bubble\n",
        "                yield (conversation, \"\")\n",
        "\n",
        "            # 3) If the conversation is done\n",
        "            elif event_type == \"thread.message.completed\":\n",
        "                yield (conversation, \"\")\n",
        "                break\n",
        "\n",
        "    return (conversation, \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Build a Gradio UI\n",
        "We'll build a chat interface that:\n",
        "1. Shows example questions (e.g. “What is NASA’s plan for Mars sample return?”)\n",
        "2. Lets the user type their own question.\n",
        "3. Displays the conversation with the agent in real-time.\n",
        "4. Has a “Clear” button to reset the thread.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "brand_theme = gr.themes.Default(\n",
        "    primary_hue=\"blue\",\n",
        "    secondary_hue=\"blue\",\n",
        "    neutral_hue=\"gray\",\n",
        ").set(\n",
        "    button_primary_background_fill=\"#005EB8\",\n",
        "    button_primary_text_color=\"#FFFFFF\",\n",
        ")\n",
        "\n",
        "with gr.Blocks(theme=brand_theme, css=\"footer {visibility: hidden;}\") as demo:\n",
        "    gr.Markdown(\"# NASA Mars Missions Chatbot\")\n",
        "    gr.Markdown(\"Ask me about NASA's Mars missions! I can also do Bing searches and call custom functions.\")\n",
        "\n",
        "    def clear_conversation():\n",
        "        global thread\n",
        "        thread = project_client.agents.create_thread()\n",
        "        return []\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        type=\"messages\",\n",
        "        examples=[\n",
        "            {\"text\": \"What is the Perseverance rover?\"},\n",
        "            {\"text\": \"When is the next NASA rocket launch?\"},\n",
        "            {\"text\": \"Summarize the Mars Sample Return mission.\"},\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    textbox = gr.Textbox(placeholder=\"Ask about NASA Mars...\")\n",
        "\n",
        "    # On example select, fill the textbox\n",
        "    def on_example(evt: gr.SelectData):\n",
        "        return evt.value[\"text\"]\n",
        "\n",
        "    chatbot.example_select(fn=on_example, inputs=None, outputs=textbox)\n",
        "\n",
        "    # Link the textbox submit to mars_agent_chat\n",
        "    (textbox\n",
        "     .submit(fn=mars_agent_chat, inputs=[textbox, chatbot], outputs=[chatbot, textbox])\n",
        "     .then(fn=lambda: \"\", outputs=textbox)\n",
        "    )\n",
        "\n",
        "    # Clear button\n",
        "    btn_clear = gr.Button(\"Clear Chat\")\n",
        "    btn_clear.click(fn=clear_conversation, inputs=None, outputs=chatbot)\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Optional: Resource Cleanup\n",
        "If you created resources just for demo (agent, thread, vector store) and want to delete them, you can do so here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # Uncomment if you want to clean up everything.\n",
        "# # WARNING: This is permanent.\n",
        "#\n",
        "# try:\n",
        "#     project_client.agents.delete_agent(agent.id)\n",
        "#     print(\"Agent deleted.\")\n",
        "# except Exception as e:\n",
        "#     print(\"Could not delete agent:\", e)\n",
        "#\n",
        "# try:\n",
        "#     project_client.agents.delete_thread(thread.id)\n",
        "#     print(\"Thread deleted.\")\n",
        "# except Exception as e:\n",
        "#     print(\"Could not delete thread:\", e)\n",
        "#\n",
        "# if vector_store_id:\n",
        "#     try:\n",
        "#         project_client.agents.delete_vector_store(vector_store_id)\n",
        "#         print(\"Vector store deleted.\")\n",
        "#     except Exception as e:\n",
        "#         print(\"Could not delete vector store:\", e)"
      ]
    }
  ]
}
